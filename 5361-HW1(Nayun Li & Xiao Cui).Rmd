---
title: "Estimate standard normal distribution function by Monte Carlo Methods"
author: "Nayun Li & Xiao Cui"
date: "2018/1/26"
output:
  pdf_document: default
  html_document: default
  documentclass: article
fontsize: 11pt
keywords: Normal Distribution Function, Monte Carlo Methods; Statistical Computing
abstract: |
  This is a report mainly designed to estimate normal distribution function by using Monte Carlo         Methods. In this report, the methodology is being showed, followed by the model realized by R codes,   the results generated by the model in variety t values and experiment times compared with the true     value of the standard normal distribution function are also included. In the end, the distribution of   bias of 100-times experiments was illustrated by box plot.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Math Equations {#sec:math}

The approximation of the standard normal distribution function of $N(0,1)$ can be shown:

$$ \Phi(t) = \int_{-\infty}^t {\frac{1}{\sqrt{2\pi}}} e^{-y^2/2}dy , $$

Which can be estimated by the Monte Carlo methods:
$$ \hat{\Phi}(t) = \frac{1}{n}\sum_{i = 1}^{n} {I(X_i \leq t) ,}$$

Where $X_i$'s are iid $N(0,1)$ variables.

## Realization by R codes
To apply the Monte Carlo methods, a function of ???distribution function??? in R was created as follows:

```{r}
#define the function of Monte Carlo methods 
distribution_function<-function(i,num){
  x     <- rnorm(num)
  sum.x <- 0
  for (j in 1:length(x)){
    if (x[j] <= i){
      sum.x <- sum.x+1
    }
  }
  return(sum.x/num)
}
```

##Test of the model (table):
In order to test the reliability of the model, different t values (0.00, 0.67, 0.84, 1.28, 1.65, 2.32, 2.58, 3.09, 3.72) and number of experiments ($10^2$,$10^3$,$10^4$) are used to apply the model. The results and the true value from the standard normal distribution function are shown together in the following table:
```{r, echo=FALSE, fig.height = 20, format = "latex"}
#define the function of Monte Carlo methods 
distribution_function<-function(i,num){
  x     <- rnorm(num)
  sum.x <- 0
  for (j in 1:length(x)){
    if (x[j] <= i){
      sum.x <- sum.x+1
    }
  }
  return(sum.x/num)
}

#print value table of the results
i.test      <- c(0, 0.67, 0.84, 1.28, 1.65, 2.32, 2.58, 3.09, 3.72)
num.test    <- c(100, 1000, 10000)
results_table <- matrix(NA, nrow = 9, ncol = 3, byrow = T, dimnames <- list(c(" t=0.0", "t=0.67", "t=0.84", "t=1.28", "t=1.65", "t=2.32", "t=2.58", "t=3.09", "t=3.72"),
                                                                            c('N=10^2', 'N=10^3', 'N=10^4')))
for(num in num.test){
    for(i in i.test){
      r_index = which(i.test == i)
      c_index = which(num.test == num)
      results_table[r_index, c_index] <- distribution_function(i, num)
    }
}
True_value <- c(pnorm(0.0), pnorm(0.67), pnorm(0.84), pnorm(1.28), pnorm(1.65), pnorm(2.32), pnorm(2.58), pnorm(3.09), pnorm(3.72))
results_table <- cbind(results_table, True_value)
knitr::kable(head(results_table), booktabs = TRUE, longtable = TRUE, font_size = 100, 
             caption = '(results_table)')
```


##Further test (Figure):
The distribution and concentration of the bias of the estimations and true values can be observed by the following box plot:

```{r, echo = FALSE}
#define the function of Monte Carlo methods 
distribution_function<-function(i,num){
  x     <- rnorm(num)
  sum.x <- 0
  for (j in 1:length(x)){
    if (x[j] <= i){
      sum.x <- sum.x+1
    }
  }
  return(sum.x/num)
}

#box plot the bias
result1 <- array()
result2 <- array()
result3 <- array()
result4 <- array()
result5 <- array()
result6 <- array()
result7 <- array()
result8 <- array()
result9 <- array()

for (k in 1:100){
  result1[k] <- distribution_function(0.00,100) - pnorm(0.00)
  result2[k] <- distribution_function(0.67,100) - pnorm(0.67)
  result3[k] <- distribution_function(0.84,100) - pnorm(0.84)
  result4[k] <- distribution_function(1.28,100) - pnorm(1.28)
  result5[k] <- distribution_function(1.65,100) - pnorm(1.65)
  result6[k] <- distribution_function(2.32,100) - pnorm(2.32)
  result7[k] <- distribution_function(2.58,100) - pnorm(2.58)
  result8[k] <- distribution_function(3.09,100) - pnorm(3.09)
  result9[k] <- distribution_function(3.72,100) - pnorm(3.72)
}

test.matrix1 <- matrix(cbind(result1,result2,result3,result4,result5,result6,result7,result8,result9),
                    ncol = 9, nrow = 100)
colnames(test.matrix1) <- c(0,0.67,0.84,1.28,1.65,2.32,2.58,3.09,3.72)

for (k in 1:100){
  result1[k] <- distribution_function(0.00, 1000) - pnorm(0.00)
  result2[k] <- distribution_function(0.67, 1000) - pnorm(0.67)
  result3[k] <- distribution_function(0.84, 1000) - pnorm(0.84)
  result4[k] <- distribution_function(1.28, 1000) - pnorm(1.28)
  result5[k] <- distribution_function(1.65, 1000) - pnorm(1.65)
  result6[k] <- distribution_function(2.32, 1000) - pnorm(2.32)
  result7[k] <- distribution_function(2.58, 1000) - pnorm(2.58)
  result8[k] <- distribution_function(3.09, 1000) - pnorm(3.09)
  result9[k] <- distribution_function(3.72, 1000) - pnorm(3.72)
}

test.matrix2 <- matrix(cbind(result1, result2, result3, result4, result5, result6, result7, result8, result9),
                     ncol = 9, nrow = 100)
colnames(test.matrix2) <- c(0, 0.67, 0.84, 1.28, 1.65, 2.32, 2.58, 3.09, 3.72)

for (k in 1:100){
  result1[k] <- distribution_function(0.00,10000) - pnorm(0.00)
  result2[k] <- distribution_function(0.67,10000) - pnorm(0.67)
  result3[k] <- distribution_function(0.84,10000) - pnorm(0.84)
  result4[k] <- distribution_function(1.28,10000) - pnorm(1.28)
  result5[k] <- distribution_function(1.65,10000) - pnorm(1.65)
  result6[k] <- distribution_function(2.32,10000) - pnorm(2.32)
  result7[k] <- distribution_function(2.58,10000) - pnorm(2.58)
  result8[k] <- distribution_function(3.09,10000) - pnorm(3.09)
  result9[k] <- distribution_function(3.72,10000) - pnorm(3.72)
}

test.matrix3 <- matrix(cbind(result1, result2, result3, result4, result5, result6, result7, result8, result9),
                     ncol = 9, nrow = 100)
colnames(test.matrix3) <- c(0, 0.67, 0.84, 1.28, 1.65, 2.32, 2.58, 3.09, 3.72)

#output the table and the figures
boxplot(test.matrix1, main = 'N=100', ylim = c(-0.12, 0.12), xlab = 't', ylab = 'Bias', col = 'red')
boxplot(test.matrix2, main = 'N=1000', ylim = c(-0.12,0.12), xlab = 't', ylab = 'Bias', col = 'blue')
boxplot(test.matrix3, main = 'N=10000', ylim = c(-0.12,0.12), xlab = 't', ylab = 'Bias', col = 'yellow')
```